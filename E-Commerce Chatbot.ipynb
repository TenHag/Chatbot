{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Load data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "nltk: Natural Language Toolkit for text processing tasks like tokenization and lemmatization.\n",
    "WordNetLemmatizer: Part of NLTK used for lemmatizing words.\n",
    "json: Library for handling JSON data.\n",
    "pickle: Python serialization library for saving Python objects.\n",
    "numpy: Library for numerical operations.\n",
    "Sequential, Dense, Activation, Dropout: Components from Keras for building a sequential neural network.\n",
    "SGD: Stochastic Gradient Descent optimizer from Keras.\n",
    "\n",
    "**Variables:**\n",
    "words: List to store unique words from the intents.\n",
    "classes: List to store unique classes (intent tags).\n",
    "documents: List to store tuples of tokenized words and their corresponding intent tags.\n",
    "ignore_words: List of punctuation marks to ignore during processing.\n",
    "Data Loading:\n",
    "Reads the content of the intents.json file, which presumably contains intents with associated patterns and tags.\n",
    "Processing Intents:\n",
    "Iterates over each intent in the loaded JSON data.\n",
    "Tokenizes each pattern within the intent, lemmatizes the words, and adds them to the words list.\n",
    "Builds a list of tuples (documents) containing tokenized words and their associated intent tags.\n",
    "Populates the classes list with unique intent tags.\n",
    "\n",
    "**Preprocessing:**\n",
    "Lemmatizes and lowercases words in the words list, removing duplicates and sorting them alphabetically.\n",
    "Sorts the classes list alphabetically.\n",
    "Data Preparation:\n",
    "Converts patterns and intents into a format suitable for training a neural network.\n",
    "Creates a bag-of-words representation for each pattern and generates corresponding output vectors.\n",
    "\n",
    "**Neural Network Model:**\n",
    "Builds a sequential neural network using Keras.\n",
    "Defines the architecture with multiple dense layers, activation functions (ReLU and Softmax), and dropout layers.\n",
    "Compiles the model using a Stochastic Gradient Descent (SGD) optimizer and categorical cross-entropy loss.\n",
    "\n",
    "**Model Training:**\n",
    "Fits the model using the prepared training data (train_x and train_y).\n",
    "\n",
    "**Model Saving:**\n",
    "Saves the trained model (chatbot_model.h5) and associated training history.\n",
    "\n",
    "**Model Loading:**\n",
    "Loads the trained model (chatbot_model.h5) for making predictions.\n",
    "\n",
    "**Utility Functions:**\n",
    "Defines utility functions like clean_up_sentence, bow, predict_class, and chatbot_response for text preprocessing and making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dhruv pujari\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import random\n",
    "\n",
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looping Through Intents and Patterns:**\n",
    "\n",
    "The code iterates over each intent in the intents dictionary.\n",
    "For each intent, it further iterates over its associated patterns.\n",
    "\n",
    "**Tokenization and Document Preparation:**\n",
    "Each pattern (sentence) within an intent is tokenized into individual words using NLTK's word_tokenize function.\n",
    "The tokenized words (w) are then extended into the words list.\n",
    "A tuple (w, intent['tag']) is appended to the documents list, which represents a document (pattern) along with its associated intent tag.\n",
    "\n",
    "**Building Vocabulary and Intent Classes:**\n",
    "Checks if the intent tag (intent['tag']) is not already in the classes list. If not, it adds the tag to classes.\n",
    "Lemmatizes each word (using WordNetLemmatizer) and converts it to lowercase, excluding words listed in ignore_words.\n",
    "Removes duplicates from the words list, sorts them alphabetically, and assigns the result back to words.\n",
    "Sorts the classes list alphabetically for consistent ordering.\n",
    "\n",
    "**Output:**\n",
    "Prints the number of documents (combinations of patterns and intents) processed.\n",
    "Prints the total number of unique intent classes (classes) and unique lemmatized words (words).\n",
    "\n",
    "**Data Serialization:**\n",
    "Serializes (saves) the words and classes lists into binary files (words.pkl and classes.pkl) using pickle.\n",
    "This allows the vocabulary (words) and intent classes (classes) to be reused for model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 documents\n",
      "22 classes ['cancel', 'confirm', 'delay', 'delivery', 'goodbye', 'greeting', 'items', 'mobile app', 'more', 'order status', 'payments', 'personal', 'profile info', 'reasons', 'refund', 'refund status', 'return', 'thanks', 'track', 'use voucher', 'user response', 'withdraw refunds']\n",
      "145 unique lemmatized words [\"'m\", \"'s\", '.', 'a', 'about', 'accept', 'account', 'address', 'agent', 'am', 'an', 'any', 'anyone', 'app', 'are', 'available', 'be', 'become', 'but', 'buy', 'bye', 'call', 'can', 'cancel', 'cancelled', 'card', 'cash', 'change', 'common', 'completed', 'confirmation', 'confirmed', 'contact', 'credit', 'credited', 'customer', 'day', 'deducted', 'delay', 'delayed', 'delete', 'delivered', 'delivery', 'did', 'do', 'doe', 'doing', 'download', 'even', 'for', 'from', 'fund', 'get', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helpline', 'hey', 'hi', 'how', 'i', 'im', 'information', 'is', 'it', 'item', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'method', 'mobile', 'money', 'more', 'my', \"n't\", 'need', 'no', 'not', 'number', 'of', 'offer', 'okay', 'only', 'order', 'password', 'pay', 'payment', 'phone', 'please', 'processed', 'profile', 'reason', 'receive', 'received', 'refund', 'refunded', 'reset', 'return', 'returned', 'say', 'see', 'sell', 'seller', 'service', 'shipping', 'should', 'status', 'successfully', 'support', 'take', 'talk', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'this', 'thnks', 'though', 'thx', 'to', 'track', 'use', 'voucher', 'wa', 'wallet', 'want', 'what', 'when', 'which', 'why', 'will', 'with', 'withdraw', 'yet', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialization:**\n",
    "\n",
    "Creates an empty list training to store processed training data.\n",
    "Initializes output_empty as a list of zeros with a length equal to the number of unique intent classes (classes).\n",
    "\n",
    "**Document Processing:**\n",
    "\n",
    "Iterates through each doc in the documents list.\n",
    "For each document (doc), initializes an empty list bag to represent the bag-of-words vector for the pattern.\n",
    "Lemmatizes and converts each word in the pattern (doc[0]) to lowercase using WordNetLemmatizer.\n",
    "Constructs the bag-of-words vector (bag) for the current pattern by checking the presence of each word (w) from the vocabulary (words) and appending 1 if the word is present, otherwise appends 0.\n",
    "\n",
    "**Intent Labeling:**\n",
    "\n",
    "Creates an output_row list initialized with zeros (output_empty) to represent the intent label vector for the current document (doc).\n",
    "Sets the appropriate index of output_row to 1 based on the position of the intent class (doc[1]) in the classes list. This step encodes the intent label in a one-hot encoded format.\n",
    "\n",
    "**Training Data Creation:**\n",
    "Appends the tuple (bag, output_row) to the training list, where bag is the bag-of-words vector and output_row is the corresponding intent label vector.\n",
    "\n",
    "**Shuffling and Array Conversion:**\n",
    "Randomly shuffles the training data to ensure that the model does not learn based on any particular order of patterns.\n",
    "Extracts the bag-of-words vectors (train_x) and intent label vectors (train_y) from the training list and converts them into NumPy arrays for compatibility with machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "training = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Iterate through documents and process each pattern\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    pattern_words = doc[0]\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    \n",
    "    # Create bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    \n",
    "    # Create output row (intent) array\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    # Append the bag of words and output row as a tuple to training\n",
    "    training.append((bag, output_row))\n",
    "\n",
    "# Shuffle the training data\n",
    "random.shuffle(training)\n",
    "\n",
    "# Extract bag and output_row arrays from the training data\n",
    "train_x = np.array([x[0] for x in training])\n",
    "train_y = np.array([x[1] for x in training])\n",
    "\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition:**\n",
    "\n",
    "Initializes a sequential model model using Sequential() from Keras.\n",
    "Adds a dense layer (Dense) with 128 units and a rectified linear unit (ReLU) activation function. The input_shape is specified as the length of the bag-of-words vector (len(train_x[0])).\n",
    "Adds a dropout layer (Dropout) with a dropout rate of 50% (0.5) to prevent overfitting.\n",
    "Adds another dense layer with 64 units and ReLU activation, followed by another dropout layer.\n",
    "\n",
    "**Output Layer:**\n",
    "\n",
    "Adds a dense output layer with the number of units equal to the length of the intent label vector (len(train_y[0])) and uses the softmax activation function to obtain probability scores across all intent classes.\n",
    "\n",
    "**Model Compilation:**\n",
    "\n",
    "Configures the model for training by specifying the loss function (categorical_crossentropy), optimizer (SGD with Nesterov momentum), and evaluation metric (accuracy).\n",
    "\n",
    "**Model Training:**\n",
    "Fits the model (model.fit) using the training data (np.array(train_x) and np.array(train_y)) for a specified number of epochs (200) and batch size (5). The training progress is displayed (verbose=1).\n",
    "\n",
    "**Model Saving:**\n",
    "Saves the trained model to a file named 'chatbot_model.h5' along with training history (hist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 3.2058 - accuracy: 0.0476\n",
      "Epoch 2/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 2.9252 - accuracy: 0.1619\n",
      "Epoch 3/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 2.6127 - accuracy: 0.2667\n",
      "Epoch 4/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.3945 - accuracy: 0.3143\n",
      "Epoch 5/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.3378 - accuracy: 0.3333\n",
      "Epoch 6/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 2.1912 - accuracy: 0.3905\n",
      "Epoch 7/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.9025 - accuracy: 0.4476\n",
      "Epoch 8/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.8462 - accuracy: 0.4381\n",
      "Epoch 9/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.6622 - accuracy: 0.4952\n",
      "Epoch 10/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.6197 - accuracy: 0.5048\n",
      "Epoch 11/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.7117 - accuracy: 0.4571\n",
      "Epoch 12/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.7105 - accuracy: 0.4476\n",
      "Epoch 13/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.3695 - accuracy: 0.5048\n",
      "Epoch 14/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.2957 - accuracy: 0.5619\n",
      "Epoch 15/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.2282 - accuracy: 0.6095\n",
      "Epoch 16/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.2653 - accuracy: 0.5714\n",
      "Epoch 17/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.2328 - accuracy: 0.6381\n",
      "Epoch 18/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.3715 - accuracy: 0.6286\n",
      "Epoch 19/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9951 - accuracy: 0.6667\n",
      "Epoch 20/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0252 - accuracy: 0.6857\n",
      "Epoch 21/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.1513 - accuracy: 0.5714\n",
      "Epoch 22/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0780 - accuracy: 0.6286\n",
      "Epoch 23/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7914 - accuracy: 0.7048\n",
      "Epoch 24/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.0656 - accuracy: 0.6095\n",
      "Epoch 25/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 1.0347 - accuracy: 0.6190\n",
      "Epoch 26/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8810 - accuracy: 0.7333\n",
      "Epoch 27/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.8743 - accuracy: 0.7143\n",
      "Epoch 28/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8768 - accuracy: 0.6762\n",
      "Epoch 29/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8817 - accuracy: 0.6667\n",
      "Epoch 30/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9813 - accuracy: 0.6762\n",
      "Epoch 31/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7277 - accuracy: 0.7048\n",
      "Epoch 32/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 1.0116 - accuracy: 0.6381\n",
      "Epoch 33/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8312 - accuracy: 0.6762\n",
      "Epoch 34/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9152 - accuracy: 0.7143\n",
      "Epoch 35/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8453 - accuracy: 0.6952\n",
      "Epoch 36/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9574 - accuracy: 0.7048\n",
      "Epoch 37/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6076 - accuracy: 0.7619\n",
      "Epoch 38/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8970 - accuracy: 0.6571\n",
      "Epoch 39/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8038 - accuracy: 0.6952\n",
      "Epoch 40/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9290 - accuracy: 0.6667\n",
      "Epoch 41/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8796 - accuracy: 0.7143\n",
      "Epoch 42/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9419 - accuracy: 0.7238\n",
      "Epoch 43/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8015 - accuracy: 0.6762\n",
      "Epoch 44/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9301 - accuracy: 0.7048\n",
      "Epoch 45/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9348 - accuracy: 0.6762\n",
      "Epoch 46/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7947 - accuracy: 0.7238\n",
      "Epoch 47/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7373 - accuracy: 0.7429\n",
      "Epoch 48/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.8391 - accuracy: 0.7333\n",
      "Epoch 49/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6908 - accuracy: 0.7238\n",
      "Epoch 50/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.7810\n",
      "Epoch 51/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5179 - accuracy: 0.7429\n",
      "Epoch 52/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4930 - accuracy: 0.8000\n",
      "Epoch 53/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7686 - accuracy: 0.7143\n",
      "Epoch 54/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5350 - accuracy: 0.8190\n",
      "Epoch 55/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6845 - accuracy: 0.7143\n",
      "Epoch 56/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.9444 - accuracy: 0.7048\n",
      "Epoch 57/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.8200 - accuracy: 0.7524\n",
      "Epoch 58/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7770 - accuracy: 0.7048\n",
      "Epoch 59/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.7619\n",
      "Epoch 60/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6538 - accuracy: 0.7714\n",
      "Epoch 61/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8476 - accuracy: 0.7333\n",
      "Epoch 62/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6041 - accuracy: 0.7810\n",
      "Epoch 63/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7504 - accuracy: 0.7810\n",
      "Epoch 64/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7437 - accuracy: 0.7333\n",
      "Epoch 65/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.7143\n",
      "Epoch 66/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6945 - accuracy: 0.7619\n",
      "Epoch 67/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4488 - accuracy: 0.8571\n",
      "Epoch 68/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5631 - accuracy: 0.7714\n",
      "Epoch 69/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7716 - accuracy: 0.7238\n",
      "Epoch 70/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7782 - accuracy: 0.7333\n",
      "Epoch 71/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.7714\n",
      "Epoch 72/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7619\n",
      "Epoch 73/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3681 - accuracy: 0.8286\n",
      "Epoch 74/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5625 - accuracy: 0.8095\n",
      "Epoch 75/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.8000\n",
      "Epoch 76/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.8000\n",
      "Epoch 77/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6403 - accuracy: 0.7714\n",
      "Epoch 78/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6050 - accuracy: 0.8000\n",
      "Epoch 79/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6661 - accuracy: 0.7143\n",
      "Epoch 80/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6364 - accuracy: 0.7524\n",
      "Epoch 81/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9041 - accuracy: 0.7429\n",
      "Epoch 82/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5904 - accuracy: 0.7810\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.7524\n",
      "Epoch 84/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8376 - accuracy: 0.7810\n",
      "Epoch 85/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6070 - accuracy: 0.7905\n",
      "Epoch 86/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7119 - accuracy: 0.7619\n",
      "Epoch 87/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7308 - accuracy: 0.7810\n",
      "Epoch 88/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7740 - accuracy: 0.7524\n",
      "Epoch 89/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6044 - accuracy: 0.7714\n",
      "Epoch 90/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6521 - accuracy: 0.8000\n",
      "Epoch 91/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7700 - accuracy: 0.7524\n",
      "Epoch 92/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7081 - accuracy: 0.7619\n",
      "Epoch 93/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8219 - accuracy: 0.6857\n",
      "Epoch 94/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5263 - accuracy: 0.8000\n",
      "Epoch 95/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6205 - accuracy: 0.8000\n",
      "Epoch 96/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.7905\n",
      "Epoch 97/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.8000\n",
      "Epoch 98/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6544 - accuracy: 0.7810\n",
      "Epoch 99/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6755 - accuracy: 0.7524\n",
      "Epoch 100/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5359 - accuracy: 0.8095\n",
      "Epoch 101/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.8286\n",
      "Epoch 102/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6635 - accuracy: 0.7429\n",
      "Epoch 103/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5740 - accuracy: 0.8000\n",
      "Epoch 104/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5300 - accuracy: 0.8381\n",
      "Epoch 105/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5347 - accuracy: 0.8000\n",
      "Epoch 106/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4719 - accuracy: 0.8286\n",
      "Epoch 107/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5324 - accuracy: 0.8286\n",
      "Epoch 108/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7108 - accuracy: 0.7429\n",
      "Epoch 109/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4573 - accuracy: 0.8762\n",
      "Epoch 110/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.8286\n",
      "Epoch 111/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7361 - accuracy: 0.7524\n",
      "Epoch 112/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4700 - accuracy: 0.8000\n",
      "Epoch 113/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.8000\n",
      "Epoch 114/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7124 - accuracy: 0.7619\n",
      "Epoch 115/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.7905\n",
      "Epoch 116/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8190\n",
      "Epoch 117/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.8667\n",
      "Epoch 118/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6950 - accuracy: 0.7905\n",
      "Epoch 119/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6451 - accuracy: 0.7714\n",
      "Epoch 120/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4532 - accuracy: 0.8286\n",
      "Epoch 121/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7512 - accuracy: 0.7714\n",
      "Epoch 122/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7355 - accuracy: 0.7810\n",
      "Epoch 123/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6967 - accuracy: 0.8095\n",
      "Epoch 124/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7869 - accuracy: 0.7238\n",
      "Epoch 125/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7905\n",
      "Epoch 126/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8190\n",
      "Epoch 127/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6384 - accuracy: 0.8190\n",
      "Epoch 128/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8095\n",
      "Epoch 129/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5523 - accuracy: 0.7714\n",
      "Epoch 130/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5920 - accuracy: 0.7810\n",
      "Epoch 131/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8381\n",
      "Epoch 132/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7810\n",
      "Epoch 133/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8190\n",
      "Epoch 134/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5605 - accuracy: 0.8000\n",
      "Epoch 135/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4581 - accuracy: 0.8095\n",
      "Epoch 136/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.8476\n",
      "Epoch 137/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8762\n",
      "Epoch 138/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4603 - accuracy: 0.7905\n",
      "Epoch 139/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8381\n",
      "Epoch 140/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.3846 - accuracy: 0.8381\n",
      "Epoch 141/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3253 - accuracy: 0.8667\n",
      "Epoch 142/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8381\n",
      "Epoch 143/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.8190\n",
      "Epoch 144/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.8190\n",
      "Epoch 145/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7810\n",
      "Epoch 146/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5988 - accuracy: 0.7810\n",
      "Epoch 147/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.8381\n",
      "Epoch 148/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.8286\n",
      "Epoch 149/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5097 - accuracy: 0.8381\n",
      "Epoch 150/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5844 - accuracy: 0.7810\n",
      "Epoch 151/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7978 - accuracy: 0.7810\n",
      "Epoch 152/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5036 - accuracy: 0.8286\n",
      "Epoch 153/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4438 - accuracy: 0.8000\n",
      "Epoch 154/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5888 - accuracy: 0.7714\n",
      "Epoch 155/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8476\n",
      "Epoch 156/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.8190\n",
      "Epoch 157/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.8286\n",
      "Epoch 158/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5727 - accuracy: 0.7714\n",
      "Epoch 159/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.3935 - accuracy: 0.8476\n",
      "Epoch 160/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4526 - accuracy: 0.8286\n",
      "Epoch 161/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.7905\n",
      "Epoch 162/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5265 - accuracy: 0.8000\n",
      "Epoch 163/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4856 - accuracy: 0.8476\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 0s 3ms/step - loss: 0.9290 - accuracy: 0.7619\n",
      "Epoch 165/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5040 - accuracy: 0.8190\n",
      "Epoch 166/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4968 - accuracy: 0.8286\n",
      "Epoch 167/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8571\n",
      "Epoch 168/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5642 - accuracy: 0.8095\n",
      "Epoch 169/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5167 - accuracy: 0.8286\n",
      "Epoch 170/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.8381\n",
      "Epoch 171/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8000\n",
      "Epoch 172/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7905\n",
      "Epoch 173/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.8286\n",
      "Epoch 174/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.8882 - accuracy: 0.8190\n",
      "Epoch 175/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6853 - accuracy: 0.7429\n",
      "Epoch 176/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6174 - accuracy: 0.8095\n",
      "Epoch 177/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6620 - accuracy: 0.7810\n",
      "Epoch 178/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.8381\n",
      "Epoch 179/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6917 - accuracy: 0.8095\n",
      "Epoch 180/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.7708 - accuracy: 0.7238\n",
      "Epoch 181/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5687 - accuracy: 0.8095\n",
      "Epoch 182/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5911 - accuracy: 0.8381\n",
      "Epoch 183/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4738 - accuracy: 0.8381\n",
      "Epoch 184/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.6441 - accuracy: 0.8000\n",
      "Epoch 185/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4492 - accuracy: 0.8476\n",
      "Epoch 186/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.8286\n",
      "Epoch 187/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8381\n",
      "Epoch 188/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6530 - accuracy: 0.7905\n",
      "Epoch 189/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6531 - accuracy: 0.7905\n",
      "Epoch 190/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5001 - accuracy: 0.8095\n",
      "Epoch 191/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6063 - accuracy: 0.8476\n",
      "Epoch 192/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5257 - accuracy: 0.8571\n",
      "Epoch 193/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.7810\n",
      "Epoch 194/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.7218 - accuracy: 0.7524\n",
      "Epoch 195/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4111 - accuracy: 0.8476\n",
      "Epoch 196/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.8286\n",
      "Epoch 197/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.8381\n",
      "Epoch 198/200\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.5174 - accuracy: 0.8571\n",
      "Epoch 199/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5222 - accuracy: 0.8476\n",
      "Epoch 200/200\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.5386 - accuracy: 0.8286\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Loading:**\n",
    "Loads the trained chatbot model (chatbot_model.h5) along with vocabulary and intent data required for intent recognition.\n",
    "\n",
    "**Text Processing:**\n",
    "Provides functions (clean_up_sentence and bow) to preprocess user input for intent prediction.\n",
    "\n",
    "**Intent Prediction:**\n",
    "Utilizes the loaded model to predict the intent of user queries and returns the predicted intent along with associated probabilities.\n",
    "\n",
    "**Response Generation:**\n",
    "Generates appropriate responses based on predicted intents, ensuring dynamic and context-aware interaction with users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('intents.json').read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words) \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(text):\n",
    "    ints = predict_class(text, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GUI Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GUI Layout:**\n",
    "\n",
    " **Window Settings:**\n",
    " Sets up a Tkinter window titled \"E-Commerce Chatbot\" with specific dimensions (width=800, height=800), background color  (bg=BG_COLOR), and other configurations.\n",
    " \n",
    " **Chat Window:**\n",
    "Creates a Text widget (ChatLog) to display chat messages. Configures it with specific fonts, colors, and disables user editing (state=DISABLED). Uses a Scrollbar to scroll through messages.\n",
    " \n",
    " **Head Label:**\n",
    "Displays a header label (head_label) at the top of the window with the text \"eBuddy\", styled with specific fonts and colors.\n",
    "\n",
    " **Separator Line:**\n",
    "Creates a horizontal line (line) below the header label to separate it from the chat window.\n",
    "\n",
    " **Input Box & Send Button:**\n",
    "Sets up an input Text widget (EntryBox) for users to type messages. Binds the <Return> key to the send function for sending messages.\n",
    "Creates a Button (SendButton) labeled \"Send\" to submit messages. Configures its appearance with specific font, colors, and a command to call the send function.\n",
    "\n",
    "    **Event Handling (send Function):\n",
    "\n",
    "    **Send Function (send):\n",
    "Defines the send function to handle user input events.\n",
    "Retrieves the message from EntryBox, processes it (e.g., removes trailing whitespace), and clears the input box.\n",
    "Displays the user's message in the chat window (ChatLog) as \"You: <message>\".\n",
    "Calls the chatbot_response function to get a response for the user's message.\n",
    "Displays the chatbot's response in the chat window as \"Bot: <response>\".\n",
    "\n",
    "    **Integration with Chatbot Logic:\n",
    "The send function integrates with a chatbot logic (assumed to be defined elsewhere, e.g., chatbot_response function) to simulate a conversation between the user and the chatbot.\n",
    "\n",
    "    **Overall Flow:\n",
    "Users interact by typing messages in EntryBox and clicking the \"Send\" button.\n",
    "Sent messages and chatbot responses are displayed in ChatLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import *\n",
    "\n",
    "BG_GRAY = \"#0a192f\"\n",
    "BG_COLOR = \"#f0f0f0\"\n",
    "TEXT_COLOR = \"#000000\"\n",
    "\n",
    "\n",
    "FONT = \"Helvetica 14\"\n",
    "FONT_BOLD = \"Helvetica 13 bold\"\n",
    "\n",
    "\n",
    "def send(event):\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"#000000\", font=(\"Verdana\", 12 ))\n",
    "\n",
    "        res = chatbot_response(msg)\n",
    "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
    "\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "        \n",
    "        \n",
    "base = Tk()\n",
    "base.title(\"E-Commerce Chatbot\")\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "base.configure(width=800, height=800, bg=BG_COLOR)\n",
    "\n",
    "\n",
    "#Create Chat window\n",
    "ChatLog = Text(base, bd=0, bg=BG_COLOR, fg=TEXT_COLOR, font=FONT_BOLD)\n",
    "ChatLog.config(state=DISABLED)\n",
    "\n",
    "head_label = Label(base, bg=BG_COLOR, fg=TEXT_COLOR, text=\"eBuddy\", font=FONT_BOLD, pady=10)\n",
    "head_label.place(relwidth=1)\n",
    "\n",
    "line = Label(base, width=450, bg=BG_GRAY)\n",
    "\n",
    "\n",
    "#Bind scrollbar to Chat window\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "ChatLog.focus()\n",
    "\n",
    "#Create Button to send message\n",
    "SendButton = Button(base, font=(\"Verdana\", 12,'bold'), text=\"Send\", width=\"12\", height=15,\n",
    "                    bd=0, bg=\"#ed9061\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command=lambda: send)\n",
    "\n",
    "#Create the box to enter message\n",
    "EntryBox = Text(base, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\", background=\"#dddddd\")\n",
    "EntryBox.focus()\n",
    "EntryBox.bind(\"<Return>\", send)\n",
    "#EntryBox.bind(\"<Return>\", send)\n",
    "\n",
    "\n",
    "\n",
    "scrollbar.place(x=775,y=6, height=800)\n",
    "line.place(x=0,y=35, height=1, width=770)\n",
    "ChatLog.place(x=5,y=40, height=700, width=770)\n",
    "EntryBox.place(x=0, y=740, height=60, width=600)\n",
    "SendButton.place(x=600, y=740, height=60, width=175)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
